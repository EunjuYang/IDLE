@name: fc

@caffe_import:
    import sys
    from caffe import layers as L

@tf_import:
    import tensorflow as tf

@attributes:
[
    {
        "name": "output_shape",
        "mandatory": "both",
        "source": "layer"
    },
    {
        "name": "use_global_stats",
        "mandatory": "both",
        "default": true,
        "source": "layer"
    },
    {
        "name": "moving_avg_fraction",
        "mandatory": "both",
        "default": 0.999,
        "source": "layer"
    },
    {
        "name": "epsilon",
        "mandatory": "both",
        "default": 1e-5,
        "source": "layer"
    },
    {
        "name": "is_scale",
        "mandatory": "both",
        "default": false,
        "source": "layer"
    },
    {
        "name": "bias_term",
        "mandatory": "both",
        "default": false,
        "source": "layer"
    },
    {
        "name": "activation",
        "mandatory": "both",
        "default": [],
        "source": "layer"
    }
]

@caffe_compile_time:
    output_shape = self.get_attr('output_shape')
    activation = self.get_attr('activation', self.activation)

    input_ = self.get_input('input')
    # weight_filler
    weight = self.get_attr('weight', 'random_normal').lower()
    if weight == 'random_normal':
        weight_filler = {'type': 'gaussian', 'mean': 0, 'std': 1}
    else: # 'zeroes'
        weight_filler = {'type': 'constant'}
    # bias_filler
    bias = self.get_attr('bias', 'zeroes').lower()
    if bias == 'random_normal':
        bias_filler = {'type': 'gaussian', 'mean': 0, 'std': 1}
    else: # 'zeroes'
        bias_filler = {'type': 'constant'}
    layer = L.InnerProduct(input_, name=self.name, num_output=output_shape, weight_filler=weight_filler,
                           bias_filler=bias_filler)

    ### activation

    for act in activation:
        # relu
        if act == 'relu':
            layer = L.ReLU(layer, name=self.name + '_relu', in_place=True)

        # # dropout
        # dropout = self.get_attr('dropout')
        # if dropout is not None:
        #     layer = L.Dropout(layer, name=self.name + '_dropout', in_place=True, dropout_ratio=dropout)

        # batch normalization
        elif act == 'batchnorm':
            use_global_stats = self.get_attr('use_global_stats', self.use_global_stats)
            moving_average_fraction = self.get_attr('moving_average_fraction', self.moving_average_Fraction)
            epsilon = self.get_attr('epsilon', self.epsilon)
            layer = L.BatchNorm(layer, name=self.name + '_batchnorm',
                                            use_global_stats=use_global_stats,
                                            moving_average_fraction=moving_average_fraction, eps=epsilon,
                                            in_place=True)

            # scale
            if self.get_attr('is_scale', self.is_scale):
                bias_term = self.get_attr('bias_term', self.bias_term)
                layer = L.Scale(layer, bias_term=bias_term, in_place=True)

    # TODO: output이름 DLMDL과 맞출 지 고민
    # self.set_output('output', layer)
    for output_name in self.outputs_list:
        self.set_output(output_name, layer)

@caffe_run_time:
    pass

@tf_compile_time:
    pass

@tf_run_time:
    def apiConsturctor(input_, output_shape, activation, mean, stddev, value, indim):
        if len(indim) == 2:
            nIn = indim[1]
        else:
            input_ = tf.reshape(input_, [indim[0], -1])
            nIn = input_.get_shape()[1].value
        weights = tf.Variable(
            tf.truncated_normal([nIn, output_shape], dtype=tf.float32, mean=mean, stddev=stddev, name='weight'))
        biases = tf.Variable(tf.constant(value, dtype=tf.float32, shape=[output_shape]), name='biase')
        fc_ = tf.add(tf.matmul(input_, weights), biases)
        # fc_ = tf.nn.relu_layer(input_, weights, biases)
        if len(activation) != 0:
            for act in activation:
                if act == 'relu':
                    fc_ = tf.nn.relu(fc_)
                else:
                    if act == 'batchnorm':
                        is_training = self.get_attr('use_global_stats', self.use_global_stats)
                        decay = self.get_attr('moving_average_fraction', self.moving_average_fraction)
                        epsilon = self.get_attr('epsilon', self.epsilon)
                        scale = False
                        center = False
                        if self.get_attr('is_scale', self.is_scale) == True:
                            scale = True
                            center = self.get_attr('bias_term', self.bias_term)
                        # batch normalization and sacling activation
                        fc_ = tf.contrib.layers.batch_norm(fc_,
                                                           decay=decay,
                                                           center=center,
                                                           scale=scale,
                                                           epsilon=epsilon,
                                                           is_training=is_training)
                    # TODO: other activation layer
                    else:
                        pass

        tf.summary.histogram('activations', fc_)
        outdim = list(fc_.get_shape()[i].value for i in xrange(len(fc_.get_shape())))
        self.set_dimension('output', outdim)
        self.set_output('output', fc_)

    output_shape = self.output_shape
    activation = self.get_attr('activation',self.activation)
    input_ = self.get_input('input')
    mean = self.get_attr('mean', 0) # not added
    stddev = self.get_attr('stddev', 1e-1) # not added
    value = self.get_attr('value', 0.0) # not added
    indim = self.get_dimension('input')

    with tf.name_scope(self.name) as scope:
        if learning_option.get("parallel", None) != "DP":
             with tf.device('/job:worker/task:%s' % self.get_attr('device')):
                apiConsturctor(input_, output_shape, activation, mean, stddev, value, indim)
        else:
            apiConsturctor(input_, output_shape, activation, mean, stddev, value, indim)