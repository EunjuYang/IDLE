@name: concat

@caffe_import:
    import sys
    from caffe import layers as L
    import numpy as np

@tf_import:
    import tensorflow as tf

@attributes:
[
    {
        "source": "layer",
        "mandatory": "both",
        "name": "concat_axis"
    }
]

@caffe_compile_time:
    concat_axis = self.get_attr('concat_axis', 'ch')
    input_ = self.get_input('input')
    indim = self.get_dimension('input')

    # Concat
    if concat_axis == 'num':
        concat_axis = 0
    elif concat_axis == 'ch':
        concat_axis = 1
    layer = L.Concat(*input_, name=self.name, axis=concat_axis)

    # TODO: output이름 DLMDL과 맞출 지고민
    self.set_output('output', layer)
    self.set_dimension('output', indim[0])
    # for output_name in self.outputs_list:
    #     print(output_name)
    #     self.set_output(output_name, layer)
    #     self.set_dimension(output_name, indim[0])


@caffe_run_time:
    pass

@tf_compile_time:
    pass

@tf_run_time:
    def apiConstructor(input_, axis):
        concat_ = tf.concat(input_, axis)
        # TODO: scalar summary error
        # tf.summary.scalar('concat', concat_)
        self.set_output('output', concat_)

    input_ = self.get_input('input')
    axis = self.get_attr('concat_axis')
    axis = 3 if axis == 'ch' else  0 # TODO: error check (e.g. "wdefw")??
    #in tensorflow, ch means 3, num means 0 (NCHW structure)

    with tf.name_scope(self.name) as scope:
        if learning_option.get("parallel", None) != "DP":
             with tf.device('/job:worker/task:%s' % self.get_attr('device')):
                apiConstructor(input_, axis)
        else:
            apiConstructor(input_, axis)
