@name: gradientdescentoptimizer

@caffe_import:


@tf_import:
    import tensorflow as tf

@attributes:
[
    {
        "default": 0.001,
        "source": "opt",
        "mandatory": "both",
        "name": "learning_rate"
    },
    {
        "default": false,
        "mandatory": "tf",
        "name": "use_locking"
    }
]


@caffe_compile_time:
    learning_rate = learning_option.get("learning_rate", self.learning_rate)

    for key in ['learning_rate', 'use_locking']:
        try:
            del learning_option[key]
        except KeyError:
            pass
    learning_option['base_lr'] = learning_rate
    learning_option['type'] = 'SGD'

@caffe_run_time:
    pass

@tf_compile_time:
    pass

@tf_run_time:
    def apiConsturctor(self, input_, learning_rate, use_locking):
        global_step = tf.train.get_or_create_global_step()
        gradopt = tf.train.GradientDescentOptimizer(learning_rate, use_locking=use_locking)
        gradopt_ = gradopt.minimize(input_, colocate_gradients_with_ops=True, global_step=global_step)
        self.set_output('output', gradopt_)
        self.set_output('global_step', global_step)

    learning_rate = learning_option.get("learning_rate", self.learning_rate)
    use_locking = learning_option.get("use_locking", self.use_locking)
    input_ = self.get_input('loss')

    with tf.name_scope(self.name) as scope:
        if learning_option.get("parallel", None) != "DP":
            with tf.device('/job:worker/task:%s' % self.get_attr('device')):
                self.apiConsturctor(input_, learning_rate, use_locking)
        else:
            self.apiConsturctor(input_, learning_rate, use_locking)
