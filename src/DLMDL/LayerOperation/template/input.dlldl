@name: input

@caffe_import:
    import numpy as np
    from scipy.misc import imread
    from scipy.misc import imresize
    import random
    import csv
    import sys
    from caffe import layers as L

@tf_import:
    import tensorflow as tf

@attributes:
[
    {
        "name": "option",
        "mandatory": "all",
        "default": "train",
        "source": "opt"
    },
    {
        "name": "file_format",
        "mandatory": "all",
        "default": "jpg",
        "source": "opt"
    },
    {
        "name": "data_path",
        "mandatory": "all",
        "source": "opt"
    },
    {
        "name": "label_path",
        "mandatory": "all",
        "source": "opt"
    },
    {
        "name": "batch_size",
        "mandatory": "all",
        "source": "opt"
    },
    {
        "name": "iteration",
        "mandatory": "all",
        "source": "opt"
    },
    {
        "name": "image_size",
        "mandatory": "all",
        "source": "layer"
    },
    {
        "name": "output_shape",
        "mandatory": "all",
        "source": "layer"
    }
]

@caffe_compile_time:
    option = learning_option.get("option", self.option)
    file_format = learning_option.get("file_format", self.file_format)
    data_path = learning_option.get("data_path", self.data_path)
    label_path = learning_option.get("label_path", self.label_path)
    batch_size = learning_option.get("batch_size", self.batch_size)
    iteration = learning_option.get("iteration", self.iteration)
    image_size = self.image_size
    output_shape = self.output_shape

    # Phase parameter setting, PHASE: 0 for trian, 1 for test
    isTrainTest = 0
    if option.lower() == "test":
        temp_include = dict(phase=caffe.TEST)
        data_path = learning_option.get("test_data_path", data_path)
        test_label_path = learning_option.get("test_label_path", label_path)
        batch_size = learning_option.get("test_batch_size", batch_size)
    elif option.lower() == "train":
        temp_include = dict(phase=caffe.TRAIN)
    elif option.lower() == "train_test":
        temp_include = dict(phase=caffe.TRAIN)
        isTrainTest = 1
    else:
        temp_include = dict(phase=caffe.TRAIN)

    # DB Data
    if file_format.lower() in ["lmdb", "leveldb"]:
        # Backend parameter setting, default value 0 (leveldb) for backend
        # Data layer setting
        image, label = L.Data(name=self.name, source=data_path,
                         batch_size=batch_size, backend=(0 if file_format.lower()=="leveldb" else 1), include=temp_include, ntop=2)

        if isTrainTest == 1:
            data_path = learning_option.get("test_data_path", data_path)
            batch_size = learning_option.get("test_batch_size", batch_size)
            temp_image, temp_label = L.Data(name=self.name, source=data_path,
                                            batch_size=batch_size,
                                            backend=(0 if file_format.lower() == "leveldb" else 1),
                                            include=dict(phase=caffe.TEST), ntop=2)
            setattr(tempNet, str(self.name) + '.image', temp_image)
            setattr(tempNet, str(self.name) + '.label', temp_label)

    # Image Data
    # TODO: HDF5 와 같은 형식을 또 다른 개별 종륭의 layer 사용 가능하나 현재 raw image 파일 형식만 들어온다고 가정
    else :
        # Read and parse the source directory
        with open(data_path+'/'+label_path, 'r') as f:
            lines = f.readlines()
        new_lines = []
        for line in lines:
            new_lines.append('/'+line.split()[0]+'.'+file_format + ' ' + line.split()[1]+'\n')
        with open(data_path+'/'+label_path.split('.')[0]+'_caffelist.txt', 'w') as f:
            f.writelines(new_lines)
            f.close()
        # Image Data layer setting
        image, label = L.ImageData(name=self.name,
                                   source=data_path + '/' + label_path.split('.')[0] + '_caffelist.txt',
                                   batch_size=batch_size, include=temp_include, ntop=2, root_folder=data_path,
                                   new_height=image_size[1], new_width=image_size[0])

        if isTrainTest == 1:
            data_path = learning_option.get("test_data_path", data_path)
            batch_size = learning_option.get("test_batch_size", batch_size)
            label_path = learning_option.get("test_label_path", label_path)

            # Read and parse the source directory
            with open(data_path + '/' + label_path, 'r') as f:
                lines = f.readlines()
            new_lines = []
            for line in lines:
                new_lines.append('/' + line.split()[0] + '.' + file_format + ' ' + line.split()[1] + '\n')
            with open(data_path + '/' + label_path.split('.')[0] + '_caffelist.txt', 'w') as f:
                f.writelines(new_lines)
                f.close()

            # Test image data layer setting
            temp_image, temp_label = L.ImageData(name=self.name,
                                                 source=data_path + '/' + label_path.split('.')[0] + '_caffelist.txt',
                                                 batch_size=batch_size, include=dict(phase=caffe.TEST), ntop=2,
                                                 root_folder=data_path, new_height=image_size[1],
                                                 new_width=image_size[0])
            setattr(tempNet, str(self.name) + '.image', temp_image)
            setattr(tempNet, str(self.name) + '.label', temp_label)

    # Record the layer output information
    self.set_output('image', image)
    self.set_output('label', label)
    self.set_dimension('image', image_size)
    try:
        if isTrainTest != 1:
            del learning_option['option']
        del learning_option['file_format']
        del learning_option['data_path']
        del learning_option['label_path']
        del learning_option['batch_size']
        del learning_option['iteration']
        learning_option['max_iter'] = iteration
    except KeyError:
        pass

    try:
        del learning_option['test_data_path']
        del learning_option['test_label_path']
        del learning_option['test_batch_size']
        learning_option['test_iter'] = learning_option.get("test_iteration", 100)
        del learning_option['test_iteration']
    except KeyError:
        pass

@caffe_run_time:
    pass

@tf_compile_time:
    pass

@tf_run_time:
    """
    input layer operation returns random batch from input image
    :return: [image, label]
    """
    '''  synthetic input for VGG
    images = tf.Variable(
        tf.random_normal([batch_size, image_size[0], image_size[1], 3], dtype=tf.float32, stddev=1e-1))
    labels = tf.Variable(tf.constant(0, dtype=tf.int32, shape=[batch_size, 1000]))
    '''

    if learning_option.get("parallel", None) != "DP":
        with tf.device('/job:worker/task:%s' % self.get_attr('device')):
            label_file = data_path + '/' + label_path
            table = tf.contrib.lookup.HashTable(
                tf.contrib.lookup.TextFileInitializer(label_file, tf.string, 0, tf.int64, 1, delimiter=" "), -1)
            filename_queue = tf.train.string_input_producer(
                tf.train.match_filenames_once("{0}/*.{1}".format(data_path, file_format)))
            image_reader = tf.WholeFileReader()
            full_image_filename, image_file = image_reader.read(filename_queue)
            image_filename = tf.string_split([full_image_filename], '/').values[-1]
            image_name = tf.string_split([image_filename], '.').values[0]
            label = tf.cast(table.lookup(image_name), tf.int64)
            image_orig = tf.image.decode_jpeg(image_file)
            image = tf.image.resize_images(image_orig, image_size)
            image.set_shape((image_size[0], image_size[1], 3))
            num_preprocess_threads = 4 # TODO: thread??
            min_queue_examples = 256
            # Create random batch using shuffle_batch method
            images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size,
                                                    num_threads=num_preprocess_threads,
                                                    capacity=min_queue_examples + 3 * batch_size,
                                                    min_after_dequeue=min_queue_examples)
            labels = tf.reshape(labels, [batch_size])
            tf.summary.image('input image', images, 10)
            self.set_output('image', images)
            self.set_output('label', labels)
    else:
        label_file = data_path + '/' + label_path
        table = tf.contrib.lookup.HashTable(
            tf.contrib.lookup.TextFileInitializer(label_file, tf.string, 0, tf.int64, 1, delimiter=" "), -1)
        filename_queue = tf.train.string_input_producer(
            tf.train.match_filenames_once("{0}/*.{1}".format(data_path, file_format)))
        image_reader = tf.WholeFileReader()
        full_image_filename, image_file = image_reader.read(filename_queue)
        image_filename = tf.string_split([full_image_filename], '/').values[-1]
        image_name = tf.string_split([image_filename], '.').values[0]
        label = tf.cast(table.lookup(image_name), tf.int64)
        image_orig = tf.image.decode_jpeg(image_file)
        image = tf.image.resize_images(image_orig, image_size)
        image.set_shape((image_size[0], image_size[1], 3))
        num_preprocess_threads = 4 # TODO: thread??
        min_queue_examples = 256
        # Create random batch using shuffle_batch method
        images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size,
                                                num_threads=num_preprocess_threads,
                                                capacity=min_queue_examples + 3 * batch_size,
                                                min_after_dequeue=min_queue_examples)
        labels = tf.reshape(labels, [batch_size])
        tf.summary.image('input image', images, 10)
        self.set_output('image', images)
        self.set_output('label', labels)
